{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent: Python asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 개념\n",
    "\n",
    "asyncio는 Python의 비동기 프로그래밍 라이브러리이다. LLM(대규모 언어 모델) 사용 시 여러 요청을 효율적으로 처리하기 위해 매우 유용하다. 비동기 프로그래밍은 I/O 집약적인 작업에서 성능을 크게 향상시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. asyncio와 async/await를 이용한 비동기 프로그래밍\n",
    "\n",
    "`async def`와 `asyncio.run`은 Python의 비동기 프로그래밍에서 중요한 요소이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## async def\n",
    "\n",
    "`async def`는 비동기 함수(코루틴)를 정의하는 구문이다. 일반 함수와 달리 이 함수는 실행 중 `await` 키워드를 사용하여 다른 비동기 작업이 완료될 때까지 실행을 일시 중단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def fetch_data():\n",
    "    print(\"데이터 가져오기 시작\")\n",
    "    # 비동기적으로 I/O 작업이 완료될 때까지 대기\n",
    "    await asyncio.sleep(2)  # 네트워크 요청이나 파일 I/O 같은 작업을 시뮬레이션\n",
    "    print(\"데이터 수신 완료\")\n",
    "    return \"가져온 데이터\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asyncio.run()\n",
    "\n",
    "`asyncio.run()`은 최상위 수준에서 비동기 함수를 실행하기 위한 함수이다.:\n",
    "\n",
    "1. 새로운 이벤트 루프를 생성하고\n",
    "2. 지정된 코루틴을 실행하고\n",
    "3. 완료되면 이벤트 루프를 닫는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 가져오기 시작\n",
      "데이터 수신 완료\n",
      "결과: 가져온 데이터\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# 위 셀에서 정의한 fetch_data 함수를 사용한다.\n",
    "async def main():\n",
    "    result = await fetch_data()\n",
    "    print(f\"결과: {result}\")\n",
    "\n",
    "# 프로그램의 진입점에서 asyncio.run()으로 비동기 코드 실행\n",
    "# Jupyter/Colab 환경에서는 asyncio.run()이 이미 실행 중인 이벤트 루프와 충돌할 수 있다.\n",
    "# 따라서 top-level await를 사용하거나 아래와 같이 실행하는 것이 좋다.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 실제 작동 방식\n",
    "\n",
    "비동기 프로그래밍이 어떻게 작동하는지 보여주는 실용적인 예제이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 A 시작\n",
      "작업 B 시작\n",
      "작업 C 시작\n",
      "작업 C 완료\n",
      "작업 B 완료\n",
      "작업 A 완료\n",
      "\n",
      "모든 작업 완료. 소요 시간: 3.00초\n",
      "결과: ['A의 결과', 'B의 결과', 'C의 결과']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def task(name, delay):\n",
    "    print(f\"작업 {name} 시작\")\n",
    "    await asyncio.sleep(delay)  # I/O 작업 시뮬레이션\n",
    "    print(f\"작업 {name} 완료\")\n",
    "    return f\"{name}의 결과\"\n",
    "\n",
    "async def main():\n",
    "    start = time.time()\n",
    "    \n",
    "    # 여러 작업을 동시에 실행\n",
    "    results = await asyncio.gather(\n",
    "        task(\"A\", 3),\n",
    "        task(\"B\", 2),\n",
    "        task(\"C\", 1)\n",
    "    )\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"\\n모든 작업 완료. 소요 시간: {end - start:.2f}초\")\n",
    "    print(f\"결과: {results}\")\n",
    "\n",
    "# 프로그램 실행\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제에서는 3개의 작업이 동시에 실행되며, 가장 긴 작업(3초)만큼만 기다리면 모든 작업이 완료된다. 순차적으로 실행했다면 3~6초가 걸린다. \n",
    "\n",
    "`asyncio.run()`(또는 `await`)을 사용하면 이벤트 루프 관리를 간소화할 수 있어 비동기 코드를 쉽게 작성할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM과 asyncio의 결합 이유\n",
    "\n",
    "LLM API 호출은 대개 네트워크 지연 시간이 발생하는 I/O 작업이다. 동기식 코드에서는 응답을 기다리는 동안 프로그램이 아무 작업도 수행하지 못하지만, 비동기 코드에서는 대기 시간 동안 다른 작업을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM 사용을 위한 asyncio 기본 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# aiohttp는 비동기 HTTP 클라이언트/서버 라이브러리이다.\n",
    "%pip install -q aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def call_llm_api(session, prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"LLM API에 비동기 요청을 보내는 함수입니다.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    json_data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    \n",
    "    async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=json_data) as response:\n",
    "        result = await response.json()\n",
    "        if \"error\" in result:\n",
    "            return f\"오류: {result['error']['message']}\"\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "async def process_multiple_prompts(prompts):\n",
    "    \"\"\"여러 프롬프트를 병렬로 처리하는 함수입니다.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session: # ClientSession을 한 번만 생성하여 재사용합니다.\n",
    "        tasks = [call_llm_api(session, prompt) for prompt in prompts]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n",
    "\n",
    "# 실행 코드\n",
    "prompts = [\n",
    "    \"파이썬의 장점을 설명해주세요.\",\n",
    "    \"머신러닝이란 무엇인가요?\",\n",
    "    \"비동기 프로그래밍의 이점은 무엇인가요?\"\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    if API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
    "        print(\"API_KEY를 설정해주세요.\")\n",
    "        return\n",
    "    \n",
    "    results = await process_multiple_prompts(prompts)\n",
    "    for prompt, result in zip(prompts, results):\n",
    "        print(f\"프롬프트: {prompt}\\n응답: {result}\\n\")\n",
    "\n",
    "# 프로그램 실행\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 고급 패턴: 속도 제한 및 재시도\n",
    "\n",
    "API에는 보통 분당 요청 제한(Rate Limit)이 있습니다. 이를 준수하고, 일시적인 네트워크 오류에 대응하기 위해 재시도 로직을 추가하면 더 안정적인 코드를 만들 수 있습니다. `tenacity` 라이브러리를 사용하면 재시도 로직을 쉽게 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "class AsyncLLMClient:\n",
    "    \"\"\"LLM API 호출을 관리하는 비동기 클라이언트 클래스입니다.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, rate_limit=10):\n",
    "        self.api_key = api_key\n",
    "        self.semaphore = asyncio.Semaphore(rate_limit)  # 동시 요청 제한\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "    async def call_api(self, session, prompt, model=\"gpt-3.5-turbo\"):\n",
    "        \"\"\"재시도 로직이 있는 API 호출 함수입니다.\"\"\"\n",
    "        async with self.semaphore:  # 속도 제한 적용\n",
    "            print(f\"'{prompt[:10]}...' 요청 시작\")\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            json_data = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "            }\n",
    "            \n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=json_data) as response:\n",
    "                # 오류 처리\n",
    "                response.raise_for_status() # 200번대 응답이 아니면 예외 발생\n",
    "                \n",
    "                result = await response.json()\n",
    "                print(f\"'{prompt[:10]}...' 요청 완료\")\n",
    "                return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    async def process_batch(self, prompts):\n",
    "        \"\"\"여러 프롬프트를 효율적으로 처리하는 함수입니다.\"\"\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tasks = [self.call_api(session, prompt) for prompt in prompts]\n",
    "            # return_exceptions=True로 설정하면 개별 작업의 예외를 모아서 반환합니다.\n",
    "            return await asyncio.gather(*tasks, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `AsyncLLMClient` 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_advanced():\n",
    "    if API_KEY == \"YOUR_OPENAI_API_KEY\":\n",
    "        print(\"API_KEY를 설정해주세요.\")\n",
    "        return\n",
    "        \n",
    "    client = AsyncLLMClient(api_key=API_KEY, rate_limit=5)\n",
    "    results = await client.process_batch(prompts)\n",
    "    \n",
    "    for prompt, result in zip(prompts, results):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"프롬프트: {prompt}\\n오류: {result}\\n\")\n",
    "        else:\n",
    "            print(f\"프롬프트: {prompt}\\n응답: {result}\\n\")\n",
    "\n",
    "await main_advanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 스트리밍 응답 처리\n",
    "\n",
    "LLM이 생성하는 텍스트를 실시간으로 받아 처리하고 싶을 때 스트리밍을 사용합니다. 사용자에게 더 빠른 피드백을 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "프롬프트: 양자 컴퓨팅에 대해 시처럼 설명해줘.\n",
      "응답: 양자 세상, 신비의 바다,  \n",
      "입자는 춤추고, 파동은 노래해.  \n",
      "0과 1의 경계 허물고,  \n",
      "양자 비트는 둘이 아냐, 세상이야.\n",
      "\n",
      "중첩의 힘, 동시에 존재,  \n",
      "한 순간에 여러 길을 걷는 그대.  \n",
      "측정의 순간, 진실이 드러나,  \n",
      "우선순위 없이 우주를 어루만져.\n",
      "\n",
      "엉킴 속의 연결, 뗄 수 없는 선,  \n",
      "멀리 있어도 하나의 마음으로.  \n",
      "양자 얽힘, 영원한 동행,  \n",
      "우리는 서로에게 빛을 전해.\n",
      "\n",
      "계산의 속도, 상상을 초월,  \n",
      "복잡한 문제를 한순간에 풀어.  \n",
      "미래의 열쇠, 새로운 시대,  \n",
      "양자 컴퓨터, 혁신의 씨앗을 뿌려.\n",
      "\n",
      "신비로운 법칙 속에 숨은 진실,  \n",
      "우리가 꿈꾸는 기술의 경계.  \n",
      "양자 세계, 무한한 가능성,  \n",
      "우리의 연구는 계속, 끝없는 여정.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import aiohttp\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경변수 로드\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def parse_stream_chunk(line):\n",
    "    \"\"\"스트리밍 응답 라인을 파싱하는 도우미 함수\"\"\"\n",
    "    if line.startswith(b'data: '):\n",
    "        line_data = line[len(b'data: '):].strip()\n",
    "        if line_data == b'[DONE]':\n",
    "            return None # 스트림 종료\n",
    "        try:\n",
    "            chunk = json.loads(line_data)\n",
    "            return chunk['choices'][0]['delta']\n",
    "        except json.JSONDecodeError:\n",
    "            return None # 잘못된 JSON 데이터\n",
    "    return None\n",
    "\n",
    "async def stream_llm_response(prompt, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"LLM의 스트리밍 응답을 처리하는 함수입니다.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    json_data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=json_data) as response:\n",
    "            print(f\"프롬프트: {prompt}\\n응답: \", end=\"\")\n",
    "            async for line in response.content:\n",
    "                if line.strip():\n",
    "                    # 스트림 데이터 파싱 및 처리\n",
    "                    chunk = parse_stream_chunk(line)\n",
    "                    if chunk and 'content' in chunk:\n",
    "                        print(chunk['content'], end='', flush=True)\n",
    "            print() # 마지막에 줄바꿈\n",
    "\n",
    "# 실행 예시\n",
    "await stream_llm_response(\"양자 컴퓨팅에 대해 시처럼 설명해줘.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 대규모 처리를 위한 작업자 풀\n",
    "\n",
    "처리해야 할 프롬프트가 수백, 수천 개에 달할 경우, `asyncio.Queue`와 작업자(worker) 패턴을 사용하면 시스템 리소스를 더 효율적으로 관리하고 안정적으로 작업을 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['인공지능(AI)의 역사는 여러 단계로 나눌 수 있으며, 다음과 같은 주요 사건과 발전이 포함됩니다.\\n\\n1. **1940년대 - 1950년대: 초기 개념과 기초 이론**  \\n   - 1943년, 연구자들은 인공지능의 기초가 되는 신경망 모델을 제안했습니다.\\n   - 1950년, 앨런 튜링은 \"튜링 테스트\"를 소개하여 기계가 인간처럼 사고할 수 있는지를 평가하는 기준을 제시했습니다.\\n   - 1956년, 다트머스 회의에서 인공지능이라는 용어가 공식적으로 사용되며, AI 연구가 본격적으로 시작되었습니다.\\n\\n2. **1960년대 - 1970년대: 탐색과 초기 적용**  \\n   - 이 시기에 초기 AI 프로그램들이 개발되었고, 문제 해결, 자연어 처리, 게임 플레이 등 다양한 분야에서 적용되었습니다.\\n   - 엘리자(ELIZA)와 같은 초기 챗봇이 등장하였고, 신경망 연구가 이어졌습니다.\\n\\n3. **1980년대: 전문가 시스템의 발전**  \\n   - 전문가 시스템이 부상하여 특정 분야에서의 의사결정을 지원하는 프로그램들이 개발되었습니다. 대표적인 예로는 MYCIN이 있습니다.\\n   - 이 시기의 성장은 AI에 대한 관심을 높였으나, 지나친 기대와 한계로 인해 \\'AI 겨울\\'이라는 연구 자원의 고갈 시기가 다가왔습니다.\\n\\n4. **1990년대: 기계 학습과 데이터 기반 접근법의 부상**  \\n   - 기계 학습과 통계적 방법론에 대한 연구가 활발해졌고, 대량의 데이터를 활용하는 방법론이 발전했습니다.\\n   - IBM의 딥 블루가 체스에서 세계 챔피언 스펜서에게 승리하는 사건이 있었고, 이는 AI의 능력을 대중에게 각인시켰습니다.\\n\\n5. **2000년대 - 현재: 딥러닝과 AI의 재도약**  \\n   - 2010년대 초반부터 딥러닝 기술이 발전하면서 이미지 인식, 자연어 처리 등 다양한 분야에서 혁신적인 성과를 이루었습니다.\\n   - 구글의 알파고가 바둑에서 세계 챔피언을 이기는 성과는 AI의 잠재력을 다시 한번 크게 부각시켰습니다.\\n   - 현재 AI는 자율주행차, 음성 인식, 추천 시스템 등 여러 분야에 광범위하게 사용되고 있으며, 윤리적 문제와 사회적 영향에 대한 논의도 활발히 진행되고 있습니다.\\n\\n이처럼 인공지능의 역사는 다양한 연구와 기술 발전의 연속으로 이루어져 있으며, 앞으로도 계속해서 변화하고 발전할 것으로 예상됩니다.', '블록체인의 핵심 기술은 다음과 같습니다:\\n\\n1. **분산 원장**: 블록체인은 여러 노드에 분산된 원장을 사용하여 데이터의 일관성과 투명성을 유지합니다. 모든 참여자가 동일한 데이터를 공유하고 검증하므로 신뢰성이 높아집니다.\\n\\n2. **암호화**: 블록체인은 거래 데이터를 보호하기 위해 암호화 기술을 사용합니다. 이를 통해 데이터의 무결성을 보장하고, 부정한 접근을 방지할 수 있습니다.\\n\\n3. **합의 알고리즘**: 블록체인 네트워크의 참여자(노드) 간에 거래의 유효성을 검증하기 위한 프로세스입니다. 비트코인은 작업 증명(Proof of Work), 이더리움은 지분 증명(Proof of Stake)과 같은 다양한 합의 알고리즘이 존재합니다.\\n\\n4. **블록 구조**: 블록체인은 연속적인 블록의 체인으로 구성되어 있으며, 각 블록은 거래 데이터와 이전 블록의 해시를 포함하여 연결됩니다. 이 구조는 거래의 변조를 어렵게 만듭니다.\\n\\n5. **스마트 계약**: 블록체인에서 자동으로 실행되는 계약으로, 특정 조건이 충족되면 거래가 자동으로 이루어집니다. 이는 중개자 없이도 안전하고 효율적인 거래를 가능하게 합니다.\\n\\n이러한 기술들이 결합되어 블록체인은 투명하고 안전한 거래를 지원하며, 다양한 산업에서 활용되고 있습니다.', '세계에서 가장 높은 산은 에베레스트 산입니다. 에베레스트 산은 네팔과 티베트의 경계에 위치하며, 해발 8,848.86 미터로 측정됩니다.', '물의 화학식은 H₂O입니다. 즉, 한 분자의 물은 두 개의 수소 원자(H)와 한 개의 산소 원자(O)로 구성되어 있습니다.', '셰익스피어의 유명한 비극 세 가지는 다음과 같습니다:\\n\\n1. **햄릿 (Hamlet)** - 덴마크의 왕자 햄릿이 부친의 원수를 갚기 위해 벌어지는 이야기를 다룬 비극입니다.\\n2. **오셀로 (Othello)** - 무어 출신의 군인 오셀로와 그의 아내 데스데모나의 비극적인 사랑 이야기를 중심으로, 질투와 배신이 주요 테마입니다.\\n3. **맥베스 (Macbeth)** - 스코틀랜드의 장군 맥베스가 권력을 갈망하며 저지르는 범죄와 그로 인한 비극적 결과를 다룬 작품입니다.\\n\\n이 외에도 셰익스피어는 많은 유명한 비극 작품을 썼습니다.']\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경변수 로드\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "class AsyncLLMClient:\n",
    "    \"\"\"비동기 LLM API 클라이언트\"\"\"\n",
    "    def __init__(self, api_key, rate_limit=5):\n",
    "        self.api_key = api_key\n",
    "        self.rate_limit = rate_limit\n",
    "        \n",
    "    async def call_api(self, session, prompt, model=\"gpt-4o-mini\"):\n",
    "        \"\"\"OpenAI API 호출\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        json_data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        async with session.post(\"https://api.openai.com/v1/chat/completions\", \n",
    "                               headers=headers, json=json_data) as response:\n",
    "            data = await response.json()\n",
    "            return data['choices'][0]['message']['content']\n",
    "\n",
    "async def worker(queue, results, client, session):\n",
    "    \"\"\"작업 큐에서 프롬프트를 처리하는 작업자 함수입니다.\"\"\"\n",
    "    while True:\n",
    "        idx, prompt = await queue.get() # 큐에서 아이템 가져오기\n",
    "        if idx is None:  # 종료 신호\n",
    "            break\n",
    "            \n",
    "        response = await client.call_api(session, prompt)\n",
    "        results[idx] = response\n",
    "        queue.task_done()\n",
    "\n",
    "async def process_large_batch(prompts, worker_count=5):\n",
    "    \"\"\"대량의 프롬프트를 작업자 풀로 처리하는 함수입니다.\"\"\"\n",
    "    client = AsyncLLMClient(API_KEY, rate_limit=worker_count)\n",
    "    queue = asyncio.Queue()\n",
    "    results = [\"\"] * len(prompts)\n",
    "    \n",
    "    # 큐에 작업 추가\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        await queue.put((i, prompt))\n",
    "    \n",
    "    # 워커 종료 신호 추가\n",
    "    for _ in range(worker_count):\n",
    "        await queue.put((None, None))\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # 작업자 시작\n",
    "        workers = [asyncio.create_task(worker(queue, results, client, session)) \n",
    "                   for _ in range(worker_count)]\n",
    "        \n",
    "        # 모든 작업 완료 대기\n",
    "        await asyncio.gather(*workers)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 대규모 프롬프트 예시 (5개로 축소)\n",
    "large_prompts = [\n",
    "    \"인공지능의 역사를 요약해줘.\",\n",
    "    \"블록체인의 핵심 기술은 무엇이야?\",\n",
    "    \"세계에서 가장 높은 산은?\",\n",
    "    \"물의 화학식은?\",\n",
    "    \"셰익스피어의 유명한 비극 3가지는?\"\n",
    "]\n",
    "\n",
    "# 실행\n",
    "final_results = await process_large_batch(large_prompts, worker_count=3)\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "LLM 애플리케이션에서 `asyncio`를 활용하면 동시에 여러 요청을 처리하고, 효율적으로 리소스를 사용하며, 응답 시간을 줄일 수 있다. 특히 여러 LLM 호출을 병렬로 처리해야 하는 대화형 애플리케이션이나 일괄 처리 시스템에서 큰 이점을 제공한다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
